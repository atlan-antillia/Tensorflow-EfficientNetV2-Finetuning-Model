<h2>EfficientNetV2-Breast-Cancer</h2>
 This is a simple Breast-Cancer Classification project based on <b>efficientnetv2</b> in <a href="https://github.com/google/automl">Brain AutoML</a>.
 <br>
Please see also our first experiment <a href="https://github.com/atlan-antillia/EfficientNet-Breast-Cancer">
EfficientNet-Breast-Cancer
</a>

<h3>1. Data Citation</h3>

 The original Breast Cancer dataset BreakHis_v1 has been taken from the following web site:<br>
<b>Laboratório Visão Robótica e Imagem</b>
<br>
<a href="https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/">
Breast Cancer Histopathological Database (BreakHis)
</a>
<br>

<h3>2. Download dataset</h3>

Please download dataset BreaKHis_V1_400X from the google drive
<a href="https://drive.google.com/file/d/1vvHUXqTsLRFaaZX_Oshk-6zNbMq1tbMD/view?usp=sharing">BreaKHis_V1_400X.zip</a>
It contains the following test and train datasets.<br>
<pre>
BreaKHis_V1_400X
  ├─test
  │  ├─benign
  │  │  └─malignant
  └─train
       ├─benign
       └─malignant
</pre>
<br>
Sample images of BreaKHis_V1_400X/malignant:<br>
<img src="./asset/Brest_Cancer_BreaKHis_V1_400X_malignant.png" width="840" height="auto">
<br> 
<br>
Sample images of BreaKHis_V1_400X/benign:<br>
<img src="./asset/Brest_Cancer_BreaKHis_V1_400X_benign.png" width="840" height="auto">
<br> 
<br>

The number of images in train and test dataset:<br>
<img src="./_BreaKHis_V1_400X_.png" width="640" height="auto">

<br>
<br>


<h3>
3. Train
</h3>
<h3>
3.1 Training script
</h3>
Please run the following bat file to train our Breast Cancer efficientnetv2 model by using
<b>BreaKHis_V1_400X/train</b>.
<pre>
./1_train.bat
</pre>
<pre>
rem 1_train.bat
rem 2024/01/14
rem optimizer=adam
python ../../../efficientnetv2/EfficientNetV2ModelTrainer.py ^
  --model_dir=./models ^
  --eval_dir=./eval ^
  --model_name=efficientnetv2-m  ^
  --data_generator_config=./data_generator.config ^
  --ckpt_dir=../../../efficientnetv2/efficientnetv2-m/model ^
  --optimizer=adam ^
  --image_size=384 ^
  --eval_image_size=480 ^
  --data_dir=./BreaKHis_V1_400X/train ^
  --data_augmentation=True ^
  --valid_data_augmentation=False ^
  --fine_tuning=True ^
  --monitor=val_loss ^
  --learning_rate=0.0004 ^
  --trainable_layers_ratio=0.4 ^
  --dropout_rate=0.2 ^
  --num_epochs=50 ^
  --batch_size=4 ^
  --patience=10 ^
  --debug=True  
</pre>
, where data_generator.config is the following:<br>
<pre>
; data_generation.config
; 2024/01/14
[training]
validation_split   = 0.2
featurewise_center = False
samplewise_center  = False
featurewise_std_normalization=False
samplewise_std_normalization =False
zca_whitening                =False
;rotation_range     = 60
rotation_range     = 90
horizontal_flip    = True
vertical_flip      = True
 
width_shift_range  = 0.2
height_shift_range = 0.2
shear_range        = 0.01
zoom_range         = [0.2, 1.0]
data_format        = "channels_last"
brightness_range   = [0.6, 1.0]
</pre>

<h3>
3.2 Training result
</h3>

This will generate a <b>best_model.h5</b> in the models folder specified by --model_dir parameter.<br>
Furthermore, it will generate a <a href="./eval/train_accuracies.csv">train_accuracies</a>
and <a href="./eval/train_losses.csv">train_losses</a> files
<br>
Training console output:<br>
<img src="./asset/train_console_output_at_epoch_33_0114.png" width="740" height="auto"><br>
<br>
Train_accuracies:<br>
<!--
<img src="./asset/Breat_Cancer_accuracies_at_epoch_25_0824.png" width="740" height="auto"><br>
-->
<img src="./eval/train_accuracies.png" width="640" height="auto"><br>

<br>
Train_losses:<br>
<!--
<img src="./asset/Breast_Cancer_train_losses_at_epoch_25_0824.png" width="740" height="auto"><br>
-->
<img src="./eval/train_losses.png" width="640" height="auto"><br>

<br>
<h3>
4. Inference
</h3>
<h3>
4.1 Inference script
</h3>
Please run the following bat file to infer the breast cancer in test images by the model generated by the above train command.<br>
<pre>
./2_inference.bat
</pre>
<pre>
rem 2_inference.bat
rem 2024/01/14
python ../../../efficientnetv2/EfficientNetV2Inferencer.py ^
  --model_name=efficientnetv2-m  ^
  --model_dir=./models ^
  --fine_tuning=True ^
  --trainable_layers_ratio=0.4 ^
  --dropout_rate=0.2 ^
  --image_path=./test/*.png ^
  --eval_image_size=480 ^
  --label_map=./label_map.txt ^
  --mixed_precision=True ^
  --infer_dir=./inference ^
  --debug=False 
</pre>
<br>
label_map.txt:
<pre>
benign
malignant
</pre>
<br>
<h3>
4.2 Sample test images
</h3>

Sample test images generated by <a href="./create_test_dataset.py">create_test_dataset.py</a> 
from <a href="./BreaKHis_V1_400X/test">BreaKHis_V1_400X/test</a>.
<br>
<img src="./asset/Breast_Cancer_BreaKHis_V1_400X_test.png" width="840" height="auto"><br>


<br>
<h3>
4.3 Inference result
</h3>
This inference command will generate <a href="./inference/inference.csv">inference result file</a>.
<br>At this time, you can see the inference accuracy for the test dataset by our trained model is very low.
More experiments will be needed to improve accuracy.<br>

<br>
Inference console output:<br>
<img src="./asset/infer_console_output_at_epoch_33_0114.png" width="740" height="auto"><br>
<br>

Inference result (inference.csv):<br>
<img src="./asset/inference_csv_at_epoch_33_0114.png" width="640" height="auto"><br>
<br>
<h3>
5. Evaluation
</h3>
<h3>
5.1 Evaluation script
</h3>
Please run the following bat file to evaluate <a href="./BreaKHis_V1_400X/test">
BreaKHis_V1_400X/test</a> by the trained model.<br>
<pre>
./3_evaluate.bat
</pre>
<pre>
rem 3_evaluate.bat
rem 2024/01/14
python ../../../efficientnetv2/EfficientNetV2Evaluator.py ^
  --model_name=efficientnetv2-m  ^
  --model_dir=./models ^
  --data_dir=./BreaKHis_V1_400X/test ^
  --evaluation_dir=./evaluation ^
  --fine_tuning=True ^
  --trainable_layers_ratio=0.4 ^
  --dropout_rate=0.2 ^
  --eval_image_size=480 ^
  --mixed_precision=True ^
  --debug=False 
</pre>
<br>

<h3>
5.2 Evaluation result
</h3>

This evaluation command will generate <a href="./evaluation/classification_report.csv">a classification report</a>
 and <a href="./evaluation/confusion_matrix.png">a confusion_matrix</a>.
<br>
<br>
Evaluation console output:<br>
<img src="./asset/evaluate_console_output_at_epoch_33_0114.png" width="740" height="auto"><br>
<br>

<br>
Classification report:<br>
<img src="./asset/classification_report_at_epoch_33_0114.png" width="640" height="auto"><br>
<br>
Confusion matrix:<br>
<img src="./evaluation/confusion_matrix.png" width="740" height="auto"><br>



